
# ================================================================================


version: '3'



services:


  # This container runs the postgresql database for mlflow.
  postgres-mlflow:
    image: postgres:13
    environment:
      POSTGRES_USER: ${POSTGRES_USER_mlflow}
      POSTGRES_PASSWORD: ${POSTGRES_USER_mlflow}
      POSTGRES_DB: ${POSTGRES_USER_mlflow}

    volumes:
      # The data folder is a named volume mounted to /var/lib/postgresql/data
      - postgres-db-volume-mlflow:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "mlflow"]
      interval: 5s
      retries: 5
    restart: always

  # This container runs the artifact storage as an S3 server.
  s3-artifact-storage:
    # The base image is minio which is a minimalistic S3 server.
    image: minio/minio
    volumes:
      # We want to serve the content from the named volume "mlflow-data" through the S3 server
      # All artifacts will be stored in this volume.
      - mlflow-data:/data
    environment:

      # The following credentials are for both login (through localhost:9001) and access to the S3 server (localhost:9000).
      MINIO_ROOT_USER: ${AWS_ACCESS_KEY_ID}   # mlflow_access
      MINIO_ROOT_PASSWORD: ${AWS_SECRET_ACCESS_KEY}

    # Launches the S3 server on port 9000 and the login server on port 9001.
    command: server /data --console-address ":9001"
    ports:
      - 9000:9000
      - 9001:9001
    healthcheck:
      test: ["CMD", "curl -I http://localhost:9000/minio/health/live"]
      interval: 5s
      retries: 5
    restart: always

  # This container creates the "data" in the S3 server, in which mlflow will later store the artifacts.
  mlflow-init:
    image: minio/mc
    depends_on:
      - s3-artifact-storage
    environment:
      MINIO_ROOT_USER: ${AWS_ACCESS_KEY_ID}   # mlflow_access
      MINIO_ROOT_PASSWORD: ${AWS_SECRET_ACCESS_KEY}

    entrypoint: >
      /bin/sh -c "
      sleep 10;
      /usr/bin/mc config host add myminio http://s3-artifact-storage:9000 $$MINIO_ROOT_USER $$MINIO_ROOT_PASSWORD;
      /usr/bin/mc mb myminio/data;
      /usr/bin/mc policy download myminio/data;
      exit 0;
      "

  # This container runs the webserver for mlflow.
  mlflow-webserver:
    build:
      context: ./dockerfiles/mlflow
    image: mlflow_server
    ports:
      - 5000:5000
    environment:
      # The following credentials allow connection to the S3 server.
      MLFLOW_S3_ENDPOINT_URL: http://s3-artifact-storage:9000
      MLFLOW_S3_IGNORE_TLS: "true"
      AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID}
      AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY}
 
    # The following command will launch the mlflow webserver on port 5000, with postgresql as the database and S3 as the artifact storage.
    # The option "--serve-artifacts" will enable the serving of artifacts through mlflow as a proxy to the S3 server.
    command: mlflow server --backend-store-uri postgresql://mlflow:mlflow@postgres-mlflow/mlflow --default-artifact-root s3://data -h 0.0.0.0 -p 5000 
    depends_on:
      - postgres-mlflow
      - s3-artifact-storage
      - mlflow-init

  # This container runs the postgresql server for airflow.
  

  jupyter:
    # docker-compose build befeore you can use the image in docker-compose up again.
    build:
      context: ./dockerfiles/jupyter
    
    ports:
      - 8888:8888
    environment:
       JUPYTER_ENABLE_LAB: "yes"
       JUPYTER_TOKEN: ${JUPYTER_TOKEN}
      # Sets the tracking uri env variable for mlflow in jupyter lab
       MLFLOW_TRACKING_URI: "http://mlflow-webserver:5000"
       GIT_PYTHON_REFRESH: "quiet"
       MLFLOW_S3_ENDPOINT_URL: "http://s3-artifact-storage:9000/"
       AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID}
       MLFLOW_S3_IGNORE_TLS: "true"
       AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY} # "mlflow_secret"

       AWS_DEFAULT_REGION: "us-east-1"

    volumes:

      # Mounts the notebooks folder
      - ./notebooks:/notebooks
      # Mounts the plugins folder.
      - ./plugins:/plugins
      # Mounts the data folder to /data
      - ./data:/data
      # Mounts the data folder to /data
      - ./:/cd4ml

    entrypoint: sh -c 'jupyter lab --ip=0.0.0.0 --port=8888 --no-browser --notebook-dir=/cd4ml --allow-root'

 

volumes:
  postgres-db-volume-mlflow:
  mlflow-data:
